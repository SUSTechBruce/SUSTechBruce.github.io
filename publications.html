<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Zhongwei Wan's Publications</title>

    <meta name="author" content="Hui Shen">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    <link rel="icon" type="image/x-icon" href="images/favicons/favicon.ico">

    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
  </head>

  <body>
        <!-- Left Background -->
        <div class="background"></div>

        <!-- Header -->
        <nav class="navbar navbar-expand-lg bg-body-tertiary header">
            <div class="container-fluid">
                <a class="navbar-brand" href="#">Bruce Wan</a>
                <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarText" aria-controls="navbarText" aria-expanded="false" aria-label="Toggle navigation">
                    <span class="navbar-toggler-icon"></span>
                </button>
                <div class="collapse navbar-collapse" id="navbarText">
                    <ul class="navbar-nav ms-auto mb-2 mb-lg-0">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html">Homeüßê</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link active" href="../publications.html">Publications</a>
                    </li>
                    <li class="nav-item">
                        <a data-bs-toggle="offcanvas" href="#contact_me" role="button" class="nav-link">Contact Me</a>
                    </li>
                    <li class="nav-item dropdown">
                        <a class="nav-link dropdown-toggle" aria-current="page" role="button" data-bs-toggle="dropdown" aria-expanded="false">
                        Free Timeüåä
                        </a>
                        <ul class="dropdown-menu">
                            <li><a class="dropdown-item" href="./misc/travel_map.html">Travel Map</a></li>
                            <!-- <li><a class="dropdown-item" href="./misc/music.html">Music</a></li>
                            <li><a class="dropdown-item" href="./misc/transfer_guide.html">Transfer Guide</a></li> -->
                            <!-- <li><a class="dropdown-item" href="./game.html">Game</a></li> -->
                        </ul>
                    </li>
                    </ul>
                </div>
            </div>
        </nav>

        <!-- Offcanvas -->
        <div class="offcanvas offcanvas-start" tabindex="-1" id="contact_me" aria-labelledby="offcanvasExampleLabel">
            <div class="offcanvas-header">
            <h5 class="offcanvas-title" id="offcanvasExampleLabel">Contact Me</h5>
            <button type="button" class="btn-close" data-bs-dismiss="offcanvas" aria-label="Close"></button>
            </div>
            <div class="offcanvas-body">
            </div>
        </div>

        <table class="container", style="width:100%;max-width:1200px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:0px">

                <!-- Publications -->
                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                    <tr>
                        <td style="padding:16px;width:100%;vertical-align:middle">
                        <h2>Official Publications</h2>
                        * denotes equal contribution
                        </td>
                    </tr>
                </tbody></table>
                <div class="row">
                    <div class="col-2">
                      <nav id="navbar-example3" class="h-100 flex-column align-items-stretch pe-4 border-end">
                        <nav class="nav nav-pills flex-column">
                          <a class="nav-link" href="#item-1">2025</a>
                          <a class="nav-link" href="#item-2">2024</a>
                          <a class="nav-link" href="#item-3">2023</a>
                        </nav>
                      </nav>
                    </div>
                  
                    <div class="col-10">
                      <div data-bs-spy="scroll" data-bs-target="#navbar-example3" data-bs-smooth-scroll="true" class="scrollspy-example-2" tabindex="0">
                        <div id="item-1">
                          <h4>2025</h4>

                            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                                <tbody>
                                    <tr>
                                        <!-- <td style="padding:10px;width:30%;vertical-align:middle">
                                            <img src='images/papers/phyx.png' width="80%">
                                        </td> -->
                                    <td style="padding:10px;width:70%;vertical-align:middle">
                                        <strong>
                                            <papertitle>
                                                <strong style="color:#1e6fff;">NeurIPS 2025</strong>     SRPO: Enhancing Multimodal LLM Reasoning via Reflection-Aware Reinforcement Learning
                                            </papertitle>
                                        </strong>
                                        <br>
                                        <strong>Zhongwei Wan</strong>, Zhihao Dou, Che Liu, Yu Zhang, Dongfei Cui, Qinjian Zhao, Hui Shen, Jing Xiong, Yi Xin, Yifan Jiang, Yangfan He, Mi Zhang, Shen Yan
                                    </td>
                                    </tr>
                                </tbody>
                            </table>


                             <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                                <tbody>
                                    <tr>
                                        <!-- <td style="padding:10px;width:30%;vertical-align:middle">
                                            <img src='images/papers/phyx.png' width="80%">
                                        </td> -->
                                    <td style="padding:10px;width:70%;vertical-align:middle">
                                        <strong>
                                            <papertitle>
                                                <strong style="color:#1e6fff;">ICLR 2025</strong>  D2O: Dynamic Discriminative Operations for Efficient Long-Context Inference of Large Language Models
                                            </papertitle>
                                        </strong>
                                        <br>
                                        <strong>Zhongwei Wan</strong>, Xinjian Wu, Yu Zhang, Yi Xin, Chaofan Tao, Zhihong Zhu, Xin Wang, Siqi Luo, Jing Xiong, Mi Zhang
                                    </td>
                                    </tr>
                                </tbody>
                            </table>


                             <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                                <tbody>
                                    <tr>
                                        <!-- <td style="padding:10px;width:30%;vertical-align:middle">
                                            <img src='images/papers/phyx.png' width="80%">
                                        </td> -->
                                    <td style="padding:10px;width:70%;vertical-align:middle">
                                        <strong>
                                            <papertitle>
                                                <strong style="color:#1e6fff;">NAACL 2025</strong>  MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context Inference
                                            </papertitle>
                                        </strong>
                                        <br>
                                        <strong>Zhongwei Wan</strong>, Hui Shen, Xin Wang, Che Liu, Zheda Mai, Mi Zhang
                                    </td>
                                    </tr>
                                </tbody>
                            </table>


                            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                                <tbody>
                                    <tr>
                                        <!-- <td style="padding:10px;width:30%;vertical-align:middle">
                                            <img src='images/papers/phyx.png' width="80%">
                                        </td> -->
                                    <td style="padding:10px;width:70%;vertical-align:middle">
                                        <strong>
                                            <papertitle>
                                                <strong style="color:#1e6fff;">ACL 2025 Findings</strong>  MEIT: Multi-Modal Electrocardiogram Instruction Tuning on Large Language Models for Report Generation
                                            </papertitle>
                                        </strong>
                                        <br>
                                        <strong>Zhongwei Wan</strong>, Che Liu, Xin Wang, Chaofan Tao, Hui Shen, Zhenwu Peng, Jie Fu, Rossella Arcucci, Huaxiu Yao, Mi Zhang
                                    </td>
                                    </tr>
                                </tbody>
                            </table>


                            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                                <tbody>
                                    <tr>
                                        <!-- <td style="padding:10px;width:30%;vertical-align:middle">
                                            <img src='images/papers/phyx.png' width="80%">
                                        </td> -->
                                    <td style="padding:10px;width:70%;vertical-align:middle">
                                        <strong>
                                            <papertitle>
                                                <strong style="color:#1e6fff;">NeurIPS 2025</strong>  Enhancing Text-to-Image Diffusion Transformer via Split-Text Conditioning
                                            </papertitle>
                                        </strong>
                                        <br>
                                        Yu Zhang, Jialei Zhou, Xinchen Li, Qi Zhang, <strong>Zhongwei Wan</strong>, Tianyu Wang, Duoqian Miao, Changwei Wang, Longbing Cao
                                    </td>
                                    </tr>
                                </tbody>
                            </table>




                               <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                                <tbody>
                                    <tr>
                                        <!-- <td style="padding:10px;width:30%;vertical-align:middle">
                                            <img src='images/papers/phyx.png' width="80%">
                                        </td> -->
                                    <td style="padding:10px;width:70%;vertical-align:middle">
                                        <strong>
                                            <papertitle>
                                                <strong style="color:#1e6fff;">NeurIPS 2025@GenAI4Health</strong>  Beyond Distillation: Pushing the Limits of Medical LLM Reasoning with Minimalist Rule-Based RL
                                            </papertitle>
                                        </strong>
                                        <br>
                                        Che Liu, Haozhe Wang, Jiazhen Pan, <strong>Zhongwei Wan</strong>, Yong Dai, Fangzhen Lin, Wenjia Bai, Daniel Rueckert, Rossella Arcucci
                                    </td>
                                    </tr>
                                </tbody>
                            </table>


                                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                                <tbody>
                                    <tr>
                                        <!-- <td style="padding:10px;width:30%;vertical-align:middle">
                                            <img src='images/papers/phyx.png' width="80%">
                                        </td> -->
                                    <td style="padding:10px;width:70%;vertical-align:middle">
                                        <strong>
                                            <papertitle>
                                                <strong style="color:#1e6fff;">ICLR 2025</strong>  SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression
                                            </papertitle>
                                        </strong>
                                        <br>
                                        Xin Wang, Yu Zheng, <strong>Zhongwei Wan</strong>, Mi Zhang
                                    </td>
                                    </tr>
                                </tbody>
                            </table>


                            
                                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                                <tbody>
                                    <tr>
                                        <!-- <td style="padding:10px;width:30%;vertical-align:middle">
                                            <img src='images/papers/phyx.png' width="80%">
                                        </td> -->
                                    <td style="padding:10px;width:70%;vertical-align:middle">
                                        <strong>
                                            <papertitle>
                                                <strong style="color:#1e6fff;">ICML 2025</strong>  ParallelComp: Parallel Long-Context Compressor for Length Extrapolation
                                            </papertitle>
                                        </strong>
                                        <br>
                                        Jing Xiong, Jianghan Shen, Chuanyang Zheng, <strong>Zhongwei Wan</strong>, Chenyang Zhao, Chiwun Yang, Fanghua Ye, Hongxia Yang, Lingpeng Kong, Ngai Wong
                                    </td>
                                    </tr>
                                </tbody>
                            </table>



                             <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                                <tbody>
                                    <tr>
                                        <!-- <td style="padding:10px;width:30%;vertical-align:middle">
                                            <img src='images/papers/phyx.png' width="80%">
                                        </td> -->
                                    <td style="padding:10px;width:70%;vertical-align:middle">
                                        <strong>
                                            <papertitle>
                                                <strong style="color:#1e6fff;">EMNLP 2025</strong>  	UNComp: Can Matrix Entropy Uncover Sparsity? -- A Compressor Design from an Uncertainty-Aware Perspective
                                            </papertitle>
                                        </strong>
                                        <br>
                                        Jing Xiong, Jianghan Shen, Fanghua Ye, Chaofan Tao, <strong>Zhongwei Wan</strong>, Jianqiao Lu, Xun Wu, Chuanyang Zheng, Zhijiang Guo, Min Yang, Lingpeng Kong, Ngai Wong
                                    </td>
                                    </tr>
                                </tbody>
                            </table>


                            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                                <tbody>
                                    <tr>
                                        <!-- <td style="padding:10px;width:30%;vertical-align:middle">
                                            <img src='images/papers/phyx.png' width="80%">
                                        </td> -->
                                    <td style="padding:10px;width:70%;vertical-align:middle">
                                        <strong>
                                            <papertitle>
                                                <strong style="color:#1e6fff;">EMNLP 2025</strong>  	Recent Advances in Large Langauge Model Benchmarks against Data Contamination: From Static to Dynamic Evaluation
                                            </papertitle>
                                        </strong>
                                        <br>
                                       Simin Chen, Yiming Chen, Zexin Li, Yifan Jiang, <strong>Zhongwei Wan</strong>, Yixin He, Dezhi Ran, Tianle Gu, Haizhou Li, Tao Xie, Baishakhi Ray
                                    </td>
                                    </tr>
                                </tbody>
                            </table>


                            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                                <tbody>
                                    <tr>
                                        <!-- <td style="padding:10px;width:30%;vertical-align:middle">
                                            <img src='images/papers/phyx.png' width="80%">
                                        </td> -->
                                    <td style="padding:10px;width:70%;vertical-align:middle">
                                        <strong>
                                            <papertitle>
                                                <strong style="color:#1e6fff;">EMNLP 2025 Findings</strong>  	Knowledge-enhanced Multimodal ECG Representation Learning with Arbitrary-Lead Inputs
                                            </papertitle>
                                        </strong>
                                        <br>
                                       Che Liu, Cheng Ouyang, <strong>Zhongwei Wan</strong>, Haozhe Wang, Wenjia Bai, Rossella Arcucci
                                    </td>
                                    </tr>
                                </tbody>
                            </table>

                            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                                <tbody>
                                    <tr>
                                        <!-- <td style="padding:10px;width:30%;vertical-align:middle">
                                            <img src='images/papers/phyx.png' width="80%">
                                        </td> -->
                                    <td style="padding:10px;width:70%;vertical-align:middle">
                                        <strong>
                                            <papertitle>
                                                <strong style="color:#1e6fff;">NAACL 2025</strong>  		SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression
                                            </papertitle>
                                        </strong>
                                        <br>
                                       Xin Wang, Samiul Alam, <strong>Zhongwei Wan</strong>, Hui Shen, Mi Zhang
                                    </td>
                                    </tr>
                                </tbody>
                            </table>


                            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                                <tbody>
                                    <tr>
                                        <!-- <td style="padding:10px;width:30%;vertical-align:middle">
                                            <img src='images/papers/phyx.png' width="80%">
                                        </td> -->
                                    <td style="padding:10px;width:70%;vertical-align:middle">
                                        <strong>
                                            <papertitle>
                                                <strong style="color:#1e6fff;">ACL 2025 Findings</strong>  		Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?
                                            </papertitle>
                                        </strong>
                                        <br>
                                       Che Liu, <strong>Zhongwei Wan</strong>, Haozhe Wang, Yinda Chen, Talha Qaiser, Chen Jin, Fariba Yousefi, Nikolay Burlutskiy, Rossella Arcucci
                                    </td>
                                    </tr>
                                </tbody>
                            </table>


                            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                                <tbody>
                                    <tr>
                                        <!-- <td style="padding:10px;width:30%;vertical-align:middle">
                                            <img src='images/papers/phyx.png' width="80%">
                                        </td> -->
                                    <td style="padding:10px;width:70%;vertical-align:middle">
                                        <strong>
                                            <papertitle>
                                                <strong style="color:#1e6fff;">ACL 2025 Findings</strong>  		Argus: Benchmarking and Enhancing Vision-Language Models for 3D Radiology Report Generation
                                            </papertitle>
                                        </strong>
                                        <br>
                                       Che Liu, <strong>Zhongwei Wan</strong>, Yuqi Wang, Hui Shen, Haozhe Wang, Kangyu Zheng, Mi Zhang, Rossella Arcucci
                                    </td>
                                    </tr>
                                </tbody>
                            </table>

                             <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                                <tbody>
                                    <tr>
                                        <!-- <td style="padding:10px;width:30%;vertical-align:middle">
                                            <img src='images/papers/phyx.png' width="80%">
                                        </td> -->
                                    <td style="padding:10px;width:70%;vertical-align:middle">
                                        <strong>
                                            <papertitle>
                                                <strong style="color:#1e6fff;">ACL 2025 Findings</strong>  		Can We Trust AI Doctors? Medical Hallucination in Large Language and Large Vision-Language Models
                                            </papertitle>
                                        </strong>
                                        <br>
                                       Zhihong Zhu, Yunyan Zhang, Xianwei Zhuang, Fan Zhang, <strong>Zhongwei Wan</strong>, Yuyan Chen, Qingqing Long, Yefeng Zheng, Xian Wu
                                    </td>
                                    </tr>
                                </tbody>
                            </table>

                            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                                <tbody>
                                    <tr>
                                        <!-- <td style="padding:10px;width:30%;vertical-align:middle">
                                            <img src='images/papers/phyx.png' width="80%">
                                        </td> -->
                                    <td style="padding:10px;width:70%;vertical-align:middle">
                                        <strong>
                                            <papertitle>
                                                <strong style="color:#1e6fff;">COLING 2025</strong>  			Evolver: Chain-of-Evolution Prompting to Boost Large Multimodal Models for Hateful Meme Detection
                                            </papertitle>
                                        </strong>
                                        <br>
                                       Jinfa Huang, Jinsheng Pan, <strong>Zhongwei Wan</strong>, Hanjia Lyu, Jiebo Luo
                                    </td>
                                    </tr>
                                </tbody>
                            </table>


                            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                                <tbody>
                                    <tr>
                                        <!-- <td style="padding:10px;width:30%;vertical-align:middle">
                                            <img src='images/papers/phyx.png' width="80%">
                                        </td> -->
                                    <td style="padding:10px;width:70%;vertical-align:middle">
                                        <strong>
                                            <papertitle>
                                                <strong style="color:#1e6fff;">TMLR 2025</strong>  				Autoregressive Models in Vision: A Survey
                                            </papertitle>
                                        </strong>
                                        <br>
                                       Jing Xiong, Gongye Liu, Lun Huang, Chengyue Wu, Taiqiang Wu, Yao Mu, Yuan Yao, Hui Shen, <strong>Zhongwei Wan</strong>, Jinfa Huang, Chaofan Tao, Shen Yan, Huaxiu Yao, Lingpeng Kong, Hongxia Yang, Mi Zhang, Guillermo Sapiro, Jiebo Luo, Ping Luo, Ngai Wong
                                    </td>
                                    </tr>
                                </tbody>
                            </table>


                            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                                <tbody>
                                    <tr>
                                        <!-- <td style="padding:10px;width:30%;vertical-align:middle">
                                            <img src='images/papers/phyx.png' width="80%">
                                        </td> -->
                                    <td style="padding:10px;width:70%;vertical-align:middle">
                                        <strong>
                                            <papertitle>
                                                <strong style="color:#1e6fff;">TMLR 2025</strong> Efficient diffusion models: A survey
                                            </papertitle>
                                        </strong>
                                        <br>
                                       Hui Shen, Jingxuan Zhang, Boning Xiong, Rui Hu, Shoufa Chen, <strong>Zhongwei Wan</strong>, Xin Wang, Yu Zhang, Zixuan Gong, Guangyin Bao, Chaofan Tao, Yongfeng Huang, Ye Yuan, Mi Zhang
                                    </td>
                                    </tr>
                                </tbody>
                            </table>




                            
                        <div id="item-2">
                          <h4>2024</h4>

                           <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                                <tbody>
                                    <tr>
                                        <!-- <td style="padding:10px;width:30%;vertical-align:middle">
                                            <img src='images/papers/phyx.png' width="80%">
                                        </td> -->
                                    <td style="padding:10px;width:70%;vertical-align:middle">
                                        <strong>
                                            <papertitle>
                                                <strong style="color:#1e6fff;">EMNLP 2024 Findings</strong> LOOK-M: Look-Once Optimization in KV Cache for Efficient Multimodal Long-Context Inference
                                            </papertitle>
                                        </strong>
                                        <br>
                                       <strong>Zhongwei Wan</strong>, Ziang Wu, Che Liu, Jinfa Huang, Zhihong Zhu, Peng Jin, Longyue Wang, Li Yuan
                                    </td>
                                    </tr>
                                </tbody>
                            </table>

                            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                                <tbody>
                                    <tr>
                                        <!-- <td style="padding:10px;width:30%;vertical-align:middle">
                                            <img src='images/papers/phyx.png' width="80%">
                                        </td> -->
                                    <td style="padding:10px;width:70%;vertical-align:middle">
                                        <strong>
                                            <papertitle>
                                                <strong style="color:#1e6fff;">TMLR 2024</strong> Efficient Large Language Models: A Survey
                                            </papertitle>
                                        </strong>
                                        <br>
                                       <strong>Zhongwei Wan</strong>, Xin Wang, Che Liu, Samiul Alam, Yu Zheng, Jiachen Liu, Zhongnan Qu, Shen Yan, Yi Zhu, Quanlu Zhang, Mosharaf Chowdhury, Mi Zhang
                                    </td>
                                    </tr>
                                </tbody>
                            </table>

                            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                                <tbody>
                                    <tr>
                                        <!-- <td style="padding:10px;width:30%;vertical-align:middle">
                                            <img src='images/papers/phyx.png' width="80%">
                                        </td> -->
                                    <td style="padding:10px;width:70%;vertical-align:middle">
                                        <strong>
                                            <papertitle>
                                                <strong style="color:#1e6fff;">EMNLP 2024</strong> DGLF: A Dual Graph-based Learning Framework for Multi-modal Sarcasm Detection
                                            </papertitle>
                                        </strong>
                                        <br>
                                       Zhihong Zhu, Kefan Shen, Zhaorun Chen, Yunyan Zhang, Yuyan Chen, Xiaoqi Jiao, <strong>Zhongwei Wan</strong>, Shaorong Xie, Wei Liu, Xian Wu, Yefeng Zheng
                                    </td>
                                    </tr>
                                </tbody>
                            </table>


                            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                                <tbody>
                                    <tr>
                                        <!-- <td style="padding:10px;width:30%;vertical-align:middle">
                                            <img src='images/papers/phyx.png' width="80%">
                                        </td> -->
                                    <td style="padding:10px;width:70%;vertical-align:middle">
                                        <strong>
                                            <papertitle>
                                                <strong style="color:#1e6fff;">NeurIPS 2024</strong> Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies
                                            </papertitle>
                                        </strong>
                                        <br>
                                      Chaofan Tao, Qian Liu, Longxu Dou, Niklas Muennighoff, <strong>Zhongwei Wan</strong>, Ping Luo, Min Lin, Ngai Wong
                                    </td>
                                    </tr>
                                </tbody>
                            </table>


                            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                                <tbody>
                                    <tr>
                                        <!-- <td style="padding:10px;width:30%;vertical-align:middle">
                                            <img src='images/papers/phyx.png' width="80%">
                                        </td> -->
                                    <td style="padding:10px;width:70%;vertical-align:middle">
                                        <strong>
                                            <papertitle>
                                                <strong style="color:#1e6fff;">NeurIPS 2024 Oral</strong> NeuroClips: Towards High-fidelity and Smooth fMRI-to-Video Reconstruction
                                            </papertitle>
                                        </strong>
                                        <br>
                                      Zixuan Gong, Guangyin Bao, Qi Zhang, <strong>Zhongwei Wan</strong>, Duoqian Miao, Shoujin Wang, Lei Zhu, Changwei Wang, Rongtao Xu, Liang Hu, Ke Liu, Yu Zhang
                                    </td>
                                    </tr>
                                </tbody>
                            </table>


                                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                                <tbody>
                                    <tr>
                                        <!-- <td style="padding:10px;width:30%;vertical-align:middle">
                                            <img src='images/papers/phyx.png' width="80%">
                                        </td> -->
                                    <td style="padding:10px;width:70%;vertical-align:middle">
                                        <strong>
                                            <papertitle>
                                                <strong style="color:#1e6fff;">NeurIPS 2024</strong> V-PETL bench: A Unified Visual Parameter-efficient Transfer Learning Benchmark
                                            </papertitle>
                                        </strong>
                                        <br>
                                      Yi Xin, Siqi Luo, Xuyang Liu, Haodi Zhou, Xinyu Cheng, Christina E Lee, Junlong Du, Haozhe Wang, MingCai Chen, Ting Liu, Guimin Hu, <strong>Zhongwei Wan</strong>, Aoxue Li, Mingyang Yi, Xiaohong Liu
                                    </td>
                                    </tr>
                                </tbody>
                            </table>



                            
                                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                                <tbody>
                                    <tr>
                                        <!-- <td style="padding:10px;width:30%;vertical-align:middle">
                                            <img src='images/papers/phyx.png' width="80%">
                                        </td> -->
                                    <td style="padding:10px;width:70%;vertical-align:middle">
                                        <strong>
                                            <papertitle>
                                                <strong style="color:#1e6fff;">NeurIPS 2024@GenAI4Health</strong> 	ClinicalBench: Can LLMs Beat Traditional ML Models in Clinical Prediction?
                                            </papertitle>
                                        </strong>
                                        <br>
                                      Canyu Chen, Jian Yu, Shan Chen, Che Liu,<strong>Zhongwei Wan</strong>, Danielle Bitterman, Fei Wang, Kai Shu
                                    </td>
                                    </tr>
                                </tbody>
                            </table>

                             <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                                <tbody>
                                    <tr>
                                        <!-- <td style="padding:10px;width:30%;vertical-align:middle">
                                            <img src='images/papers/phyx.png' width="80%">
                                        </td> -->
                                    <td style="padding:10px;width:70%;vertical-align:middle">
                                        <strong>
                                            <papertitle>
                                                <strong style="color:#1e6fff;">ICML 2024</strong> 	Zero-Shot ECG Classification with Multimodal Learning and Test-time Clinical Knowledge Enhancement
                                            </papertitle>
                                        </strong>
                                        <br>
                                      Che Liu, <strong>Zhongwei Wan</strong>, Cheng Ouyang, Anand Shah, Wenjia Bai, Rossella Arcucci
                                    </td>
                                    </tr>
                                </tbody>
                            </table>


                            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                                <tbody>
                                    <tr>
                                        <!-- <td style="padding:10px;width:30%;vertical-align:middle">
                                            <img src='images/papers/phyx.png' width="80%">
                                        </td> -->
                                    <td style="padding:10px;width:70%;vertical-align:middle">
                                        <strong>
                                            <papertitle>
                                                <strong style="color:#1e6fff;">ICML 2024@AI4Science</strong> 	Structure-based Drug Design Benchmark: Do 3D Methods Really Dominate?
                                            </papertitle>
                                        </strong>
                                        <br>
                                      Kangyu Zheng, Yingzhou Lu, Zaixi Zhang, <strong>Zhongwei Wan</strong>, Yao Ma, Marinka Zitnik, Tianfan Fu
                                    </td>
                                    </tr>
                                </tbody>
                            </table>


                            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                                <tbody>
                                    <tr>
                                        <!-- <td style="padding:10px;width:30%;vertical-align:middle">
                                            <img src='images/papers/phyx.png' width="80%">
                                        </td> -->
                                    <td style="padding:10px;width:70%;vertical-align:middle">
                                        <strong>
                                            <papertitle>
                                                <strong style="color:#1e6fff;">ECCV 2024@CADL</strong> 	Famba-V: Fast Vision Mamba with Cross-Layer Token Fusion ( <strong><font color='#b70000'>Best Paper AwardüèÜ</strong></font>)
                                            </papertitle>
                                        </strong>
                                        <br>
                                      Hui Shen, <strong>Zhongwei Wan</strong>, Xin Wang, Mi Zhang
                                    </td>
                                    </tr>
                                </tbody>
                            </table>

                            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                                <tbody>
                                    <tr>
                                        <!-- <td style="padding:10px;width:30%;vertical-align:middle">
                                            <img src='images/papers/phyx.png' width="80%">
                                        </td> -->
                                    <td style="padding:10px;width:70%;vertical-align:middle">
                                        <strong>
                                            <papertitle>
                                                <strong style="color:#1e6fff;">IEEE Internet Computing Magazine 2024</strong> 	The Internet of Things in the Era of Generative AI: Vision and Challenges ( <strong><font color='#b70000'>Best Paper AwardüèÜ</strong></font>)
                                            </papertitle>
                                        </strong>
                                        <br>
                                      Xin Wang, <strong>Zhongwei Wan</strong>, Arvin Hekmati, Mingyu Zong, Samiul Alam, Mi Zhang, Bhaskar Krishnamachari
                                    </td>
                                    </tr>
                                </tbody>
                            </table>

                            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                                <tbody>
                                    <tr>
                                        <!-- <td style="padding:10px;width:30%;vertical-align:middle">
                                            <img src='images/papers/phyx.png' width="80%">
                                        </td> -->
                                    <td style="padding:10px;width:70%;vertical-align:middle">
                                        <strong>
                                            <papertitle>
                                                <strong style="color:#1e6fff;">ICASSP 2024</strong> 	ETP: Learning Transferable ECG Representations via ECG-Text Pre-training
                                            </papertitle>
                                        </strong>
                                        <br>
                                     <strong>Che Liu*, Zhongwei Wan*</strong>, Sibo Cheng, Mi Zhang, Rossella Arcucci
                                    </td>
                                    </tr>
                                </tbody>
                            </table>

                            
                        </div>
                        <div id="item-3">
                            <h4>2023</h4>


                             <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                                <tbody>
                                    <tr>
                                        <!-- <td style="padding:10px;width:30%;vertical-align:middle">
                                            <img src='images/papers/phyx.png' width="80%">
                                        </td> -->
                                    <td style="padding:10px;width:70%;vertical-align:middle">
                                        <strong>
                                            <papertitle>
                                                <strong style="color:#1e6fff;">NeurIPS 2023</strong> 		Med-UniC: Unifying Cross-Lingual Medical Vision-Language Pre-Training by Diminishing Bias
                                            </papertitle>
                                        </strong>
                                        <br>
                                     <strong>Zhongwei Wan</strong>, Che Liu, Mi Zhang, Jie Fu, Benyou Wang, Sibo Cheng, Lei Ma, C√©sar Quilodr√°n-Casas, Rossella Arcucci
                                    </td>
                                    </tr>
                                </tbody>
                            </table>


                             <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                                <tbody>
                                    <tr>
                                        <!-- <td style="padding:10px;width:30%;vertical-align:middle">
                                            <img src='images/papers/phyx.png' width="80%">
                                        </td> -->
                                    <td style="padding:10px;width:70%;vertical-align:middle">
                                        <strong>
                                            <papertitle>
                                                <strong style="color:#1e6fff;">ACM TOIS 2023</strong> 		Spatio-Temporal Contrastive Learning Enhanced GNNs for Session-based Recommendation
                                            </papertitle>
                                        </strong>
                                        <br>
                                     <strong>Zhongwei Wan</strong>, Xin Liu, Benyou Wang, Jiezhong Qiu, Boyu Li, Ting Guo, Guangyong Chen, Yang Wang
                                    </td>
                                    </tr>
                                </tbody>
                            </table>
                           


                             <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                                <tbody>
                                    <tr>
                                        <!-- <td style="padding:10px;width:30%;vertical-align:middle">
                                            <img src='images/papers/phyx.png' width="80%">
                                        </td> -->
                                    <td style="padding:10px;width:70%;vertical-align:middle">
                                        <strong>
                                            <papertitle>
                                                <strong style="color:#1e6fff;">EMNLP 2022</strong> 		G-map: general memory-augmented pre-trained language model for domain tasks
                                            </papertitle>
                                        </strong>
                                        <br>
                                     <strong>Zhongwei Wan</strong>,  Yichun Yin, Wei Zhang, Jiaxin Shi, Lifeng Shang, Guangyong Chen, Xin Jiang, Qun Liu
                                    </td>
                                    </tr>
                                </tbody>
                            </table>
                           
                           







                          </div>
                      </div>
                    </div>
                </div>





              </td>
            </tr>
        </table>

        <button id="backToTop" class="back-to-top">
            <svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                <polyline points="18 15 12 9 6 15"></polyline>
            </svg>
        </button>
    
        <script>
            document.addEventListener("DOMContentLoaded", function () {
                const navbar = document.getElementById("navbar-example3");
                const offsetTop = navbar.offsetTop;
        
                window.addEventListener("scroll", function () {
                    if (window.scrollY >= offsetTop) {
                        navbar.style.position = "fixed";
                        navbar.style.top = "100px";
                    } else {
                        navbar.style.position = "static";
                    }
                });
            });
            
            const backToTopButton = document.getElementById('backToTop');
            window.addEventListener('scroll', () => {
                if (window.scrollY > 300) {
                    backToTopButton.classList.add('show');
                } else {
                    backToTopButton.classList.remove('show');
                }
            });

            backToTopButton.addEventListener('click', () => {
                window.scrollTo({
                    top: 0,
                    behavior: 'smooth'
                });
            });
        </script>

        <script src="./js/bootstrap.bundle.min.js"></script>
    </body>