<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Zhongwei Wan</title>

    <meta name="author" content="Zhongwei Wan">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="keywords" content="Zhongwei Wan, The Ohio State University, Computer Science, AI, Machine Learning, Multimodal Models">
    <meta name="description" content="Zhongwei Wan is a PhD student at the The Ohio State University specializing in Efficient LLMs and LLMs Reasoning.">

    <link rel="icon" type="image/x-icon" href="images/favicons/favicon.ico">
    
    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">

    
  </head>

  <body>

    <!-- Left Background -->
    <div class="background"></div>


    <!-- Header -->
    <nav class="navbar navbar-expand-lg bg-body-tertiary header">
        <div class="container-fluid">
          <a class="navbar-brand" href="#">Bruce Wan</a>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarText" aria-controls="navbarText" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
          </button>
          <div class="collapse navbar-collapse" id="navbarText">
              <ul class="navbar-nav ms-auto mb-2 mb-lg-0">
              <li class="nav-item">
                  <a class="nav-link active" href="../index.html">Homeüßê</a>
              </li>
              <li class="nav-item">
                  <a class="nav-link" href="../publications.html">Publicationsüìë</a>
              </li>
              <li class="nav-item">
                <a data-bs-toggle="offcanvas" href="#contact_me" role="button" class="nav-link">Contact Meüëãüèª</a>
              </li>
              <li class="nav-item dropdown">
                  <a class="nav-link dropdown-toggle" aria-current="page" role="button" data-bs-toggle="dropdown" aria-expanded="false">
                  Free Timeüåä
                  </a>
                  <ul class="dropdown-menu">
                      <li><a class="dropdown-item" href="./misc/travel_map.html">Travel Map</a></li>
                      <!-- <li><a class="dropdown-item" href="./misc/music.html">Music</a></li>
                      <li><a class="dropdown-item" href="./misc/transfer_guide.html">Transfer Guide</a></li> -->
                      <!-- <li><a class="dropdown-item" href="./game.html">Game</a></li> -->
                  </ul>
              </li>
              </ul>
          </div>
        </div>
    </nav>

    <!-- Offcanvas -->
    <div class="offcanvas offcanvas-start" tabindex="-1" id="contact_me" aria-labelledby="offcanvasExampleLabel">
      <div class="offcanvas-header">
        <h5 class="offcanvas-title" id="offcanvasExampleLabel">Contact Me</h5>
        <button type="button" class="btn-close" data-bs-dismiss="offcanvas" aria-label="Close"></button>
      </div>
      <div class="offcanvas-body">
        <strong>Email-1:</strong><br>
        wan.512[at]osu.edu
        <hr>
        <strong>Email-2:</strong><br>
        wanzhongwei666[at]gmail.com
      </div>
    </div>

    <table class="container", style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">

        <!-- Introduction -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
                <td style="padding:2.5%;width:37%;max-width:37%">
                  <a href="images/zhongweiwan.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/zhongweiwan.jpg" class="hoverZoomLink"></a>
                </td>
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Zhongwei Wan ‰∏á‰∏≠Â®Å
                </p>
                <!-- <p>
                  I'm a Final year Ph.D. student (graduate in 2026.5) at the <a href="https://umich.edu/" class="a_umich">The Ohio State University</a> in Computer Science and Engineering, advised by Prof. <a href="https://web.eecs.umich.edu/~zmao/">Mi Zhang</a>. I have a broad interest in <strong>Efficient Machine Learning Systems</strong> and <strong>Generative Models</strong>. Feel free to contact me if you are interested in my work or want to discuss a potential collaboration :P
                </p>
                <p>
                  Previously, I obtained my Bachelor of Science in Computer & Information Science from <a href="https://www.osu.edu/" class="a_osu">The Ohio State University</a>üå∞, where I was advised by Prof. <a href="https://mi-zhang.github.io/">Mi Zhang</a> and Prof. <a href="https://sites.google.com/view/jingbomeng">Jingbo Meng</a>. I also work closely with Prof. <a href="https://www.eee.hku.hk/~nwong/">Ngai Wong</a> at <a href="https://www.hku.hk/" class="a_hku">The University of Hong Kongüìñ</a>.
                </p> -->
                <p>
                I am a final-year Ph.D. student (expected graduation in May 2026) at <a href="https://cse.osu.edu/" class="a_osu">The Ohio State University</a>üå∞ in Computer Science and Engineering, advised by <a href="https://mi-zhang.github.io/">Prof. Mi Zhang</a>. My research focuses on <strong>Foundation Model Reasoning</strong> (Test-time Scaling, RL for LLMs/MLLMs), <strong>Efficient Foundation Models</strong> (Long-context LLMs, MLLMs, VLAs), and <strong>Domain-specific Foundation Models</strong>. My work has been published at <strong>NeurIPS</strong>, <strong>ICLR</strong>, <strong>ICML</strong>, <strong>EMNLP</strong>, <strong>ACL</strong>, <strong>NAACL</strong>, <strong>TMLR</strong>, <strong>ACM TOIS</strong>, and <strong>ICASSP</strong>, and I am the recipient of the <a href="https://cse.osu.edu/news/2024/10/mi-zhang-and-students-won-eccv-2024-workshop-best-paper-award">ECCV CADL Workshop Best Paper Award üèÜ</a> and the <a href="https://www.computer.org/publications/best-paper-award-winners">IEEE Internet Computing Magazine Best Paper Award üèÜ</a>.
                </p>

            <p>
                Previously, I worked as a research scientist intern at <a href="https://seed.bytedance.com/en/">Bytedance Seed</a>  (Multimodal Pre-training Team) at San Jose, <a href="https://ailab.tencent.com/ailab/en/index/">Tencent AI Lab</a> (NLP Group), and Noah‚Äôs Ark Lab (Speech and Language Group). I received my M.S. degree from the <a href="https://english.ucas.ac.cn/">University of Chinese Academy of Sciences</a> and my B.S. degree from <a href="https://www.sustech.edu.cn/en/">Southern University of Science and Technology</a>. Feel free to contact me if you are interested in my work or potential collaborations :P
                </p>

                <p style="text-align:center">
                    <!-- <a href="data/Hui_Shen_CV.pdf">CV</a> &nbsp;/&nbsp; -->
                    <a href="https://scholar.google.com/citations?user=EVj1cNoAAAAJ&hl">Google Scholar</a> &nbsp;/&nbsp;
                    <a href="https://www.linkedin.com/in/zhongwei-wan-2977031a1/?locale=en_US">LinkedIn</a> &nbsp;/&nbsp;
                    <!-- <a href="https://www.instagram.com/qinyan_hai/">Instagram</a> &nbsp;/&nbsp; -->
                    <a href="https://x.com/wanzhongwei666">X</a> &nbsp;/&nbsp;
                    <a href="https://github.com/SUSTechBruce">GitHub</a>
                </p>
                </td>

            </tr>
        </tbody></table>


        <!-- News -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:16px;width:100%;vertical-align:middle">
                    <h2>What's New</h2>
                    <div class="scrollable-list border p-3">
                      <ul>
                        <li>
                          <strong>May 2025:</strong> Two papers have been accepted by NeurIPS 2025, please see <a href="https://srpo.pages.dev/">SRPO for MLLMs reasoning</a>.
                        </li>
                        <li>
                          <strong>August 2025:</strong> Three papers have been accepted by EMNLP 2025.
                        </li>
                        <li>
                          <strong>May 2025:</strong> Released paper <a href="https://arxiv.org/pdf/2505.15929">PhyX: Does Your Model Have the "Wits" for Physical Reasoning?</a>, the dataset of PhyX at <a href="https://huggingface.co/datasets/Cloudriver/PhyX">Huggingface Dataset</a>, and the <a href="https://github.com/NastyMarcus/PhyX">evaluation code</a>.
                        </li>
                        <li>
                          <strong>May 2025:</strong> Four papers have been accepted by ACL 2025.
                        </li>
                        <li>
                          <strong>Feb 2025:</strong> One papers have been accepted by ICML 2025.
                        </li>
                        <li>
                          <strong>Feb 2025:</strong> Two papers have been accepted by ICLR 2025, please see <a href="https://github.com/AIoT-MLSys-Lab/D2O">D2O for optimizing Long-context LLMs</a>.
                        </li>

                        <li>
                          <strong>Nov 2024:</strong> I joined <a href="https://seed.bytedance.com/en/">Bytedance Seed</a> in the U.S. as a Research Scientist Intern.
                        </li>

                        
       
                      </ul>
                    </div>
                </td>
            </tr>
        </tbody></table>

        <!-- Education -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:16px;width:100%;vertical-align:middle">
            <h2>Education</h2>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tbody>
                <tr>
                    <td style="padding:10px;width:20%;vertical-align:middle">
                        <img src='./images/OSU_Logo.png' width="80%">
                    </td>
                <td style="padding:10px;width:70%;vertical-align:middle">
                  <p>
                    <strong>The Ohio State University, United States</strong><br />
                    Ph.D. Student in Computer & Information Science <br />
                    <!-- <strong>Summa Cum Laude</strong><br /> -->
                    Advised by Prof. <a href="https://mi-zhang.github.io/">Mi Zhang</a> 
                  </p>  
                </td>
                </tr>
            </tbody>
        </table>
        <br>
      
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tbody>
                <tr>
                    <td style="padding:10px;width:20%;vertical-align:middle">
                        <img src='./images/UCAS.png' width="80%">
                    </td>
                <td style="padding:10px;width:70%;vertical-align:middle">
                  <p>
                    <strong>University of the Chinese Academy of Sciences, China</strong><br />
                    MPhil in Control Science & Engineering <br />
                    <!-- <strong>Summa Cum Laude</strong><br /> -->
                    (2020.9 - 2023.6)
                  </p>  
                </td>
                </tr>
            </tbody>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tbody>
                <tr>
                    <td style="padding:10px;width:20%;vertical-align:middle">
                        <img src='./images/sustech_logo_square.png' width="80%">
                    </td>
                <td style="padding:10px;width:70%;vertical-align:middle">
                  <p>
                    <strong>Southern University of Science and Technology, China</strong><br />
                    B.S. in Computer Science<br />
                    <!-- <strong>Summa Cum Laude</strong><br /> -->
                    (2016.9 - 2020.6)
                  </p>  
                </td>
                </tr>
            </tbody>
        </table>
          </td>
          </tr>
      </tbody></table>


              <!-- Services -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:16px;width:100%;vertical-align:middle">
              <h2>Services</h2>
              <ul>
                <li>
                    <strong>Reviewer:</strong> ICML, ICLR, NeurIPS, ACL, EMNLP, NAACL,TKDE, TMLR
                </li>
              </ul>
            </td>
            </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:16px;width:100%;vertical-align:middle">
              <h2>Talks</h2>
             <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tbody>
                <tr>
                    <td style="padding:10px;width:30%;vertical-align:middle">
                        <img src='images/talks.png' width="80%">
                    </td>
                <td style="padding:10px;width:70%;vertical-align:middle">
                    <a href="https://x.com/MedaiStanford/status/1768686034299064736/photo/1">
                    <papertitle>
                        Towards Fundamental Biomedical AI: Integrating Vision, Language, and Signals
                    </papertitle>
                    </a>
                    <br>
                    <strong>Zhongwei Wan, Chen Liu</strong>, 2024.3
                  <br>
                </td>
                </tr>
            </tbody>
        </table>
              </ul>
            </td>
            </tr>
        </tbody></table>


      <!-- Featured Publications -->
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
              <h2>Recent Research</h2>
              See Official <a href="./publications.html">Publications</a>.

              </td>
          </tr>
      </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tbody>
              <tr>
                  <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/papers/srpo.png' width="80%">
                  </td>
              <td style="padding:10px;width:70%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2506.01713">
                  <papertitle>
                    SRPO: Enhancing Multimodal LLM Reasoning via Reflection-Aware Reinforcement Learning
                  </papertitle>
                  </a>
                  <br>
                  <strong>Zhongwei Wan</strong>,
                  Zhihao Dou, Che Liu, Yu Zhang, Dongfei Cui, Qinjian Zhao, Hui Shen, Jing Xiong, Yi Xin, Yifan Jiang, Yangfan He, Mi Zhang, Shen Yan
                  <br>
                  <strong style="color:#1e6fff;">NeurIPS 2025</strong>
                  <br>
                  <a href="https://srpo.pages.dev/">Project Page</a>
                  /
                  <a href="https://huggingface.co/datasets/bruce360568/SRPO_RL_datasets">Data</a>
                  /
                  <a href="https://github.com/SUSTechBruce/SRPO_MLLMs">Code</a>
                  /
                  <a href="https://arxiv.org/abs/2506.01713">Paper</a>
              </td>
              </tr>
            </tbody>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tbody>
                <tr>
                    <td style="padding:10px;width:30%;vertical-align:middle">
                        <img src='images/papers/PTA-GRPO.png' width="80%">
                    </td>
                <td style="padding:10px;width:70%;vertical-align:middle">
                    <a href="https://openreview.net/">
                    <papertitle>
                        Plan Then Action: High-level Planning Guidance Reinforcement Learning for LLM Reasoning
                    </papertitle>
                    </a>
                    <br>
                    Zhihao Dou<strong>*</strong>, Qinjian Zhao<strong>*</strong>, <strong>Zhongwei Wan*</strong>, Dinggen Zhang, Weida Wang, Towsif Raiyan, Benteng Chen, Qingtao Pan, Yang Ouyang, Zhiqiang Gao, Shufei Zhang, Sumon Biswas
                    <br>
                    <em>Under Review 2025, <strong>*Co-first Author</strong></em>
                  <br>
                    <!-- <a href="https://github.com/AIoT-MLSys-Lab/Efficient-Diffusion-Model-Survey">GitHub Repo</a>
                    /
                    <a href="https://arxiv.org/pdf/2502.06805">Paper</a> -->
                                      <a href="https://github.com/SUSTechBruce/PTA-GRPO">Code</a>
                  /
                  <a href="https://arxiv.org/abs/xxxx.xxxxx">Paper</a>
                </td>
                </tr>
            </tbody>
        </table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tbody>
              <tr>
                  <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/papers/A1.png' width="80%">
                  </td>
              <td style="padding:10px;width:70%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2509.15148">
                  <papertitle>
                    A1: Asynchronous Test-Time Scaling via Conformal Prediction
                  </papertitle>
                  </a>
                  <br>
                  Jing Xiong, Qiujiang Chen, Fanghua Ye, <strong>Zhongwei Wan</strong>, Chuanyang Zheng, Chenyang Zhao, Hui Shen, Alexander Hanbo Li, Chaofan Tao, Haochen Tan, Haoli Bai, Lifeng Shang, Lingpeng Kong, Ngai Wong
                  <br>
                  <em>Under Review 2025</em>
                  <br>
                  <a href="https://github.com/menik1126/asynchronous-test-time-scaling">Code</a>
                  /
                  <a href="https://arxiv.org/abs/2509.15148">Paper</a>
              </td>
              </tr>
            </tbody>
        </table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tbody>
              <tr>
                  <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/papers/phyx.png' width="80%">
                  </td>
              <td style="padding:10px;width:70%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2505.15929">
                  <papertitle>
                    PhyX: Does Your Model Have the "Wits" for Physical Reasoning?
                  </papertitle>
                  </a>
                  <br>
                 Hui Shen, Taiqiang Wu, Qi Han, Yunta Hsieh, Jizhou Wang, Yuyue Zhang, Yuxin Cheng, Zijian Hao, Yuansheng Ni, Xin Wang, <strong>Zhongwei Wan</strong>, Kai Zhang, Wendong Xu, Jing Xiong, Ping Luo, Wenhu Chen, Chaofan Tao, Zhuoqing Mao, Ngai Wong
                  <br>
                  <em>Under Review 2025</em>
                  <br>
                  <a href="https://phyx-bench.github.io/">Code</a>
                  /
                  <a href="https://arxiv.org/abs/2505.15929">Paper</a>
              </td>
              </tr>
            </tbody>
        </table>


           <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tbody>
              <tr>
                  <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/papers/R2LLM.png' width="80%">
                  </td>
              <td style="padding:10px;width:70%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2505.15929">
                  <papertitle>
                   R2-LLMs: Enhancing Test-Time Scaling of Large Language Models with Hierarchical Retrieval-Augmented MCTS
                  </papertitle>
                  </a>
                  <br>
                 Zhihao Dou<strong>*</strong>, <strong>Zhongwei Wan*</strong>, Dongfei Cui, Xin Wang, Jing Xiong, Haokun Lin, Chaofan Tao, Shen Yan, Mi Zhang
                 <br>
                  <em>Under Review 2025, <strong>*Co-first Author</strong></em>
                  <br>
                  <a href="https://arxiv.org/abs/2505.15929">Paper</a>
              </td>
              </tr>
            </tbody>
        </table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tbody>
              <tr>
                  <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/papers/yu_nips_2025.png' width="80%">
                  </td>
              <td style="padding:10px;width:70%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2505.19261">
                  <papertitle>
                    Enhancing Text-to-Image Diffusion Transformer via Split-Text Conditioning
                  </papertitle>
                  </a>
                  <br>
                 Yu Zhang, Jialei Zhou, Xinchen Li, Qi Zhang, <strong>Zhongwei Wan</strong>, Tianyu Wang, Duoqian Miao, Changwei Wang, Longbing Cao
                  <br>
                  <strong style="color:#1e6fff;">NeurIPS 2025</strong>
                  <br>
                  <a href="https://github.com/SUSTechBruce/xxxx">Code</a>
                  /
                  <a href="https://arxiv.org/abs/2505.19261">Paper</a>
              </td>
              </tr>
            </tbody>
        </table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tbody>
              <tr>
                  <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/papers/med_rl.png' width="80%">
                  </td>
              <td style="padding:10px;width:70%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2505.17952">
                  <papertitle>
                    Beyond Distillation: Pushing the Limits of Medical LLM Reasoning with Minimalist Rule-Based RL
                  </papertitle>
                  </a>
                  <br>
                 Che Liu, Haozhe Wang, Jiazhen Pan, <strong>Zhongwei Wan</strong>, Yong Dai, Fangzhen Lin, Wenjia Bai, Daniel Rueckert, Rossella Arcucci
                  <br>
                  <em>Under Review 2025</em>
                  <br>
                  <a href="https://cheliu-computation.github.io/AlphaM/">Code</a>
                  /
                  <a href="https://arxiv.org/abs/2505.17952">Paper</a>
              </td>
              </tr>
            </tbody>
        </table>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tbody>
              <tr>
                  <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/papers/code RLpng.png' width="80%">
                  </td>
              <td style="padding:10px;width:70%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2412.20367">
                  <papertitle>
                    Enhancing Code LLMs with Reinforcement Learning in Code Generation
                  </papertitle>
                  </a>
                  <br>
                Junqiao Wang, Zeng Zhang, Yangfan He, Zihao Zhang, Yuyang Song, Tianyu Shi, Yuchen Li, Hengyuan Xu, Kunyu Wu, Xin Yi, <strong>Zhongwei Wan</strong>, Xinhang Yuan, Kuan Lu, Menghao Huo, Tang Jingqun, Guangwu Qian, Keqin Li, Qiuwu Chen, Lewei He
                  <br>
                  <em>Technical Report 2025</em>
                  <br>
                  <a href="https://arxiv.org/abs/2412.20367">Paper</a>
              </td>
              </tr>
            </tbody>
        </table>




           <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tbody>
              <tr>
                  <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/papers/Lowcost.png' width="80%">
                  </td>
              <td style="padding:10px;width:70%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2507.21858">
                  <papertitle>
                    Low-Cost Test-Time Adaptation for Robust Video Editing
                  </papertitle>
                  </a>
                  <br>
                 Jianhui Wang, Yinda Chen, Yangfan He, Xinyuan Song, Yi Xin, Dapeng Zhang, <strong>Zhongwei Wan</strong>, Bin Li, Rongchao Zhang
                  <br>
                  <em>Under Review 2025</em>
                  <br>
                  <a href="https://arxiv.org/abs/2507.21858">Paper</a>
              </td>
              </tr>
            </tbody>
        </table>





              <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tbody>
              <tr>
                  <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/papers/uncomp.png' width="80%">
                  </td>
              <td style="padding:10px;width:70%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2410.03090">
                  <papertitle>
                    UNComp: Can Matrix Entropy Uncover Sparsity? -- A Compressor Design from an Uncertainty-Aware Perspective
                  </papertitle>
                  </a>
                  <br>
                Jing Xiong, Jianghan Shen, Fanghua Ye, Chaofan Tao, <strong>Zhongwei Wan</strong>, Jianqiao Lu, Xun Wu, Chuanyang Zheng, Zhijiang Guo, Min Yang, Lingpeng Kong, Ngai Wong
                  <br>
                  <strong style="color:#1e6fff;">EMNLP 2025</strong>
                  <br>
                  <a href="https://github.com/menik1126/UNComp">Code</a>
                  /
                  <a href="https://arxiv.org/abs/2410.03090">Paper</a>
              </td>
              </tr>
            </tbody>
        </table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tbody>
              <tr>
                  <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/papers/EMNLP_survey.png' width="80%">
                  </td>
              <td style="padding:10px;width:70%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2502.17521">
                  <papertitle>
                    Recent Advances in Large Langauge Model Benchmarks against Data Contamination: From Static to Dynamic Evaluation
                  </papertitle>
                  </a>
                  <br>
                Simin Chen, Yiming Chen, Zexin Li, Yifan Jiang, <strong>Zhongwei Wan</strong>, Yixin He, Dezhi Ran, Tianle Gu, Haizhou Li, Tao Xie, Baishakhi Ray
                  <br>
                  <strong style="color:#1e6fff;">EMNLP 2025</strong>
                  <br>
                  <a href="https://github.com/SeekingDream/Static-to-Dynamic-LLMEval">Code</a>
                  /
                  <a href="https://arxiv.org/abs/2502.17521">Paper</a>
              </td>
              </tr>
            </tbody>
        </table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tbody>
              <tr>
                  <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/papers/EMNLP_che.png' width="80%">
                  </td>
              <td style="padding:10px;width:70%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2502.17900">
                  <papertitle>
                    Knowledge-enhanced Multimodal ECG Representation Learning with Arbitrary-Lead Inputs
                  </papertitle>
                  </a>
                  <br>
               Che Liu, Cheng Ouyang, <strong>Zhongwei Wan</strong>, Haozhe Wang, Wenjia Bai, Rossella Arcucci
                  <br>
                  <strong style="color:#1e6fff;">EMNLP 2025 Findings</strong>
                  <br>
                  <a href="https://arxiv.org/abs/2502.17900">Paper</a>
              </td>
              </tr>
            </tbody>
        </table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tbody>
              <tr>
                  <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/papers/swing-bench.png' width="80%">
                  </td>
              <td style="padding:10px;width:70%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2505.23932">
                  <papertitle>
                    SwingArena: Competitive Programming Arena for Long-context GitHub Issue Solving
                  </papertitle>
                  </a>
                  <br>
               Wendong Xu, Jing Xiong, Chenyang Zhao, Qiujiang Chen, Haoran Wang, Hui Shen, <strong>Zhongwei Wan</strong>, Jianbo Dai, Taiqiang Wu, He Xiao, Chaofan Tao, Z Morley Mao, Ying Sheng, Zhijiang Guo, Hongxia Yang, Bei Yu, Lingpeng Kong, Quanquan Gu, Ngai Wong
                  <br>
                  <em>Under Review 2025</em>
                  <br>
                  <a href="https://swing-bench.github.io/">Code</a>
                  /
                  <a href="https://arxiv.org/abs/2505.23932">Paper</a>
              </td>
              </tr>
            </tbody>
        </table>





        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tbody>
              <tr>
                  <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/papers/ICLR_D2O.png' width="80%">
                  </td>
              <td style="padding:10px;width:70%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2406.13035">
                  <papertitle>
                    D2O: Dynamic Discriminative Operations for Efficient Long-Context Inference of Large Language Models
                  </papertitle>
                  </a>
                  <br>
               <strong>Zhongwei Wans</strong>, Xinjian Wu, Yu Zhang, Yi Xin, Chaofan Tao, Zhihong Zhu, Xin Wang, Siqi Luo, Jing Xiong, Mi Zhang
                  <br>
                  <strong style="color:#1e6fff;">ICLR 2025</strong>
                  <br>
                  <a href="https://github.com/AIoT-MLSys-Lab/d2o">Code</a>
                  /
                  <a href="https://arxiv.org/abs/2406.13035">Paper</a>
              </td>
              </tr>
            </tbody>
        </table>


              <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tbody>
              <tr>
                  <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/papers/svdllm.png' width="80%">
                  </td>
              <td style="padding:10px;width:70%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2403.07378">
                  <papertitle>
                   SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression
                  </papertitle>
                  </a>
                  <br>
               Xin Wang, Yu Zheng, <strong>Zhongwei Wan</strong>, Mi Zhang
               <br>
                  <strong style="color:#1e6fff;">ICLR 2025</strong>
                  <br>
                  <a href="https://github.com/AIoT-MLSys-Lab/SVD-LLM">Code</a>
                  /
                  <a href="https://arxiv.org/abs/2403.07378">Paper</a>
              </td>
              </tr>
            </tbody>
        </table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tbody>
              <tr>
                  <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/papers/ICML2025.png' width="80%">
                  </td>
              <td style="padding:10px;width:70%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2502.14317">
                  <papertitle>
                   ParallelComp: Parallel Long-Context Compressor for Length Extrapolation
                  </papertitle>
                  </a>
                  <br>
               Jing Xiong, Jianghan Shen, Chuanyang Zheng, <strong>Zhongwei Wan</strong>, Chenyang Zhao, Chiwun Yang, Fanghua Ye, Hongxia Yang, Lingpeng Kong, Ngai Wong
               <br>
                  <strong style="color:#1e6fff;">ICML 2025</strong>
                  <br>
                  <a href="https://github.com/AIoT-MLSys-Lab/SVD-LLM">Code</a>
                  /
                  <a href="https://arxiv.org/abs/2502.14317">Paper</a>
              </td>
              </tr>
            </tbody>
        </table>
        


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tbody>
              <tr>
                  <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/papers/meda.png' width="80%">
                  </td>
              <td style="padding:10px;width:70%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2502.17599">
                  <papertitle>
                   MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context Inference
                  </papertitle>
                  </a>
                  <br>
               <strong>Zhongwei Wan</strong>, Hui Shen, Xin Wang, Che Liu, Zheda Mai, Mi Zhang
               <br>
                  <strong style="color:#1e6fff;">NAACL 2025</strong>
                  <br>
                  <a href="https://github.com/AIoT-MLSys-Lab/SVD-LLM">Code</a>
                  /
                  <a href="https://arxiv.org/abs/2502.17599">Paper</a>
              </td>
              </tr>
            </tbody>
        </table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tbody>
              <tr>
                  <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/papers/svd2.png' width="80%">
                  </td>
              <td style="padding:10px;width:70%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2503.12340">
                  <papertitle>
                  SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression
                  </papertitle>
                  </a>
                  <br>
               Xin Wang, Samiul Alam, <strong>Zhongwei Wan</strong>, Hui Shen, Mi Zhang
               <br>
                  <strong style="color:#1e6fff;">NAACL 2025</strong>
                  <br>
                  <a href="https://github.com/AIoT-MLSys-Lab/SVD-LLM">Code</a>
                  /
                  <a href="https://arxiv.org/abs/2503.12340">Paper</a>
              </td>
              </tr>
            </tbody>
        </table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tbody>
              <tr>
                  <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/papers/meit.png' width="80%">
                  </td>
              <td style="padding:10px;width:70%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2403.04945">
                  <papertitle>
                  MEIT: Multi-Modal Electrocardiogram Instruction Tuning on Large Language Models for Report Generation
                  </papertitle>
                  </a>
                  <br>
               <strong>Zhongwei Wan</strong>, Che Liu, Xin Wang, Chaofan Tao, Hui Shen, Zhenwu Peng, Jie Fu, Rossella Arcucci, Huaxiu Yao, Mi Zhang
               <br>
                  <strong style="color:#1e6fff;">ACL 2025 Findings</strong>
                  <br>
                  <a href="https://github.com/AIoT-MLSys-Lab/MEIT">Code</a>
                  /
                  <a href="https://arxiv.org/abs/2403.04945">Paper</a>
              </td>
              </tr>
            </tbody>
        </table>


         <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tbody>
              <tr>
                  <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/papers/Can_che.png' width="80%">
                  </td>
              <td style="padding:10px;width:70%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2410.13523">
                  <papertitle>
                 Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?
                  </papertitle>
                  </a>
                  <br>
              Che Liu, <strong>Zhongwei Wan</strong>, Haozhe Wang, Yinda Chen, Talha Qaiser, Chen Jin, Fariba Yousefi, Nikolay Burlutskiy, Rossella Arcucci
               <br>
                  <strong style="color:#1e6fff;">ACL 2025 Findings</strong>
                  <br>
                  <a href="https://arxiv.org/abs/2410.13523">Paper</a>
              </td>
              </tr>
            </tbody>
        </table>



        
         <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tbody>
              <tr>
                  <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/papers/augus.png' width="80%">
                  </td>
              <td style="padding:10px;width:70%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2406.07146">
                  <papertitle>
                 Argus: Benchmarking and Enhancing Vision-Language Models for 3D Radiology Report Generation
                  </papertitle>
                  </a>
                  <br>
              Che Liu, <strong>Zhongwei Wan</strong>, Yuqi Wang, Hui Shen, Haozhe Wang, Kangyu Zheng, Mi Zhang, Rossella Arcucci
               <br>
                  <strong style="color:#1e6fff;">ACL 2025 Findings</strong>
                  <br>
                  <a href="https://arxiv.org/abs/2406.07146">Paper</a>
              </td>
              </tr>
            </tbody>
        </table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tbody>
              <tr>
                  <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/papers/ACL_zhupng.png' width="80%">
                  </td>
              <td style="padding:10px;width:70%;vertical-align:middle">
                  <a href="https://aclanthology.org/2025.findings-acl.350/">
                  <papertitle>
                 Can We Trust AI Doctors? Medical Hallucination in Large Language and Large Vision-Language Models
                  </papertitle>
                  </a>
                  <br>
              Zhihong Zhu, Yunyan Zhang, Xianwei Zhuang, Fan Zhang, <strong>Zhongwei Wan</strong>, Yuyan Chen, Qingqing Long, Yefeng Zheng, Xian Wu
               <br>
                  <strong style="color:#1e6fff;">ACL 2025 Findings</strong>
                  <br>
                  <a href="https://aclanthology.org/2025.findings-acl.350/">Paper</a>
              </td>
              </tr>
            </tbody>
        </table>




        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tbody>
              <tr>
                  <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/papers/longemotion.png' width="80%">
                  </td>
              <td style="padding:10px;width:70%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2509.07403">
                  <papertitle>
                 LongEmotion: Measuring Emotional Intelligence of Large Language Models in Long-Context Interaction
                  </papertitle>
                  </a>
                  <br>
              Weichu Liu, Jing Xiong, Yuxuan Hu, Zixuan Li, Minghuan Tan, Ningning Mao, Chenyang Zhao, <strong>Zhongwei Wan</strong>, Chaofan Tao, Wendong Xu, Hui Shen, Chengming Li, Lingpeng Kong, Ngai Wong
               <br>
                  <em>Under Review 2025</em>
                  <br>
                 <a href="https://github.com/LongEmotion/LongEmotion/tree/main/LongEmotion">Code</a>
                  /
                  <a href="https://arxiv.org/abs/2509.07403">Paper</a>
              </td>
              </tr>
            </tbody>
        </table>





        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tbody>
              <tr>
                  <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/papers/TMLR_auto_regression.png' width="80%">
                  </td>
              <td style="padding:10px;width:70%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2411.05902">
                  <papertitle>
                 Autoregressive Models in Vision: A Survey
                  </papertitle>
                  </a>
                  <br>
              Jing Xiong, Gongye Liu, Lun Huang, Chengyue Wu, Taiqiang Wu, Yao Mu, Yuan Yao, Hui Shen, <strong>Zhongwei Wan</strong>, Jinfa Huang, Chaofan Tao, Shen Yan, Huaxiu Yao, Lingpeng Kong, Hongxia Yang, Mi Zhang, Guillermo Sapiro, Jiebo Luo, Ping Luo, Ngai Wong
               <br>
                  <strong style="color:#1e6fff;">TMLR 2025</strong>
                  <br>
                   <a href="https://github.com/ChaofanTao/Autoregressive-Models-in-Vision-Survey">Code</a>
                  /
                  <a href="https://arxiv.org/abs/2411.05902">Paper</a>
              </td>
              </tr>
            </tbody>
        </table>



          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tbody>
              <tr>
                  <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/papers/efficient_diffusion_survey.png' width="80%">
                  </td>
              <td style="padding:10px;width:70%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2502.06805">
                  <papertitle>
                   Efficient diffusion models: A survey
                  </papertitle>
                  </a>
                  <br>
              Hui Shen, Jingxuan Zhang, Boning Xiong, Rui Hu, Shoufa Chen, <strong>Zhongwei Wan</strong>, Xin Wang, Yu Zhang, Zixuan Gong, Guangyin Bao, Chaofan Tao, Yongfeng Huang, Ye Yuan, Mi Zhang
               <br>
                  <strong style="color:#1e6fff;">TMLR 2025</strong>
                  <br>
                   <a href="https://github.com/ChaofanTao/Autoregressive-Models-in-Vision-Survey">Code</a>
                  /
                  <a href="https://arxiv.org/abs/2502.06805">Paper</a>
              </td>
              </tr>
            </tbody>
        </table>




              <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tbody>
              <tr>
                  <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/papers/Colling 2025.png' width="80%">
                  </td>
              <td style="padding:10px;width:70%;vertical-align:middle">
                  <a href="https://aclanthology.org/2025.coling-main.489/">
                  <papertitle>
                   Evolver: Chain-of-Evolution Prompting to Boost Large Multimodal Models for Hateful Meme Detection
                  </papertitle>
                  </a>
                  <br>
             Jinfa Huang, Jinsheng Pan, <strong>Zhongwei Wan</strong>, Hanjia Lyu, Jiebo Luo
               <br>
                  <strong style="color:#1e6fff;">COLING 2025</strong>
                  <br>
                   <a href="https://github.com/ChaofanTao/Autoregressive-Models-in-Vision-Survey">Code</a>
                  /
                  <a href="https://aclanthology.org/2025.coling-main.489/">Paper</a>
              </td>
              </tr>
            </tbody>
        </table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tbody>
              <tr>
                  <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/papers/zixuan.png' width="80%">
                  </td>
              <td style="padding:10px;width:70%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2406.18139">
                  <papertitle>
                 UncertaintyRAG: Span-Level Uncertainty Enhanced Long-Context Modeling for Retrieval-Augmented Generation
                  </papertitle>
                  </a>
                  <br>
              Zixuan Li, Jing Xiong, Fanghua Ye, Chuanyang Zheng, Xun Wu, Jianqiao Lu, <strong>Zhongwei Wan</strong>, Xiaodan Liang, Chengming Li, Zhenan Sun, Lingpeng Kong, Ngai Wong
               <br>
                  <em>Under Review 2024</em>
                  <br>
                  <a href="https://github.com/SUSTechBruce/LOOK-M">Code</a>
                  /
                  <a href="https://arxiv.org/abs/2406.18139">Paper</a>
              </td>
              </tr>
            </tbody>
        </table>


              <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tbody>
              <tr>
                  <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/papers/lookm.png' width="80%">
                  </td>
              <td style="padding:10px;width:70%;vertical-align:middle">
                  <a href="https://aclanthology.org/2025.coling-main.489/">
                  <papertitle>
                   LOOK-M: Look-Once Optimization in KV Cache for Efficient Multimodal Long-Context Inference
                  </papertitle>
                  </a>
                  <br>
           <strong>Zhongwei Wan</strong>, Ziang Wu, Che Liu, Jinfa Huang, Zhihong Zhu, Peng Jin, Longyue Wang, Li Yuan
               <br>
                  <strong style="color:#1e6fff;">EMNLP 2024 Findings</strong>
                  <br>
                   <a href="https://github.com/ChaofanTao/Autoregressive-Models-in-Vision-Survey">Code</a>
                  /
                  <a href="https://aclanthology.org/2025.coling-main.489/">Paper</a>
              </td>
              </tr>
            </tbody>
        </table>


<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tbody>
              <tr>
                  <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/papers/EMNLP24_zhu.png' width="80%">
                  </td>
              <td style="padding:10px;width:70%;vertical-align:middle">
                  <a href="ttps://aclanthology.org/2024.emnlp-main.170/">
                  <papertitle>
                  DGLF: A Dual Graph-based Learning Framework for Multi-modal Sarcasm Detection
                  </papertitle>
                  </a>
                  <br>
           Zhihong Zhu, Kefan Shen, Zhaorun Chen, Yunyan Zhang, Yuyan Chen, Xiaoqi Jiao, <strong>Zhongwei Wan</strong>, Shaorong Xie, Wei Liu, Xian Wu, Yefeng Zheng
               <br>
                  <strong style="color:#1e6fff;">EMNLP 2024</strong>
                  <br>
                  <a href="ttps://aclanthology.org/2024.emnlp-main.170/">Paper</a>
              </td>
              </tr>
            </tbody>
        </table>

        


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tbody>
                <tr>
                    <td style="padding:10px;width:30%;vertical-align:middle">
                        <img src='images/papers/fambav.png' width="80%">
                    </td>
                <td style="padding:10px;width:70%;vertical-align:middle">
                    <a href="https://www.arxiv.org/pdf/2409.09808">
                    <papertitle>
                        Famba-V: Fast Vision Mamba with Cross-Layer Token Fusion
                    </papertitle>
                    </a>
                    <br>
                    Hui Shen,
                    <strong>Zhongwei Wan</strong>,
                    Xin Wang,
                    Mi Zhang
                    <br>
                    <em><strong style="color:#1e6fff;">ECCV 2024</strong> @ Computational Aspects of Deep Learning</em> <strong><font color='#b70000'> (Best Paper AwardüèÜ)</strong></font>
                    <br>
                    <a href="https://github.com/AIoT-MLSys-Lab/Famba-V">Code</a>
                    /
                    <a href="https://www.arxiv.org/pdf/2409.09808">Paper</a>
                </td>
                </tr>
            </tbody>
        </table>


        
              <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tbody>
              <tr>
                  <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/papers/vocab.png' width="80%">
                  </td>
              <td style="padding:10px;width:70%;vertical-align:middle">
                  <a href="https://proceedings.neurips.cc/paper_files/paper/2024/hash/cf5a019ae9c11b4be88213ce3f85d85c-Abstract-Conference.html">
                  <papertitle>
                   Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies
                  </papertitle>
                  </a>
                  <br>
                Chaofan Tao, Qian Liu, Longxu Dou, Niklas Muennighoff, <strong>Zhongwei Wan</strong>, Ping Luo, Min Lin, Ngai Wong
               <br>
                  <strong style="color:#1e6fff;">NeurIPS 2024</strong>
                  <br>
                   <a href="https://huggingface.co/spaces/sail/scaling-with-vocab-demo">Code</a>
                  /
                  <a href="https://proceedings.neurips.cc/paper_files/paper/2024/hash/cf5a019ae9c11b4be88213ce3f85d85c-Abstract-Conference.html">Paper</a>
              </td>
              </tr>
            </tbody>
        </table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tbody>
              <tr>
                  <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/papers/nips24_oral.png' width="80%">
                  </td>
              <td style="padding:10px;width:70%;vertical-align:middle">
                  <a href="https://proceedings.neurips.cc/paper_files/paper/2024/hash/5c594bf6223b67109441c9e0c97542ed-Abstract-Conference.html">
                  <papertitle>
                   NeuroClips: Towards High-fidelity and Smooth fMRI-to-Video Reconstruction
                  </papertitle>
                  </a>
                  <br>
                Zixuan Gong, Guangyin Bao, Qi Zhang, <strong>Zhongwei Wan</strong>, Duoqian Miao, Shoujin Wang, Lei Zhu, Changwei Wang, Rongtao Xu, Liang Hu, Ke Liu, Yu Zhang
               <br>
                  <strong style="color:#1e6fff;">NeurIPS 2024 Oral</strong>
                  <br>
                   <a href="https://github.com/gongzix/NeuroClips">Code</a>
                  /
                  <a href="https://proceedings.neurips.cc/paper_files/paper/2024/hash/5c594bf6223b67109441c9e0c97542ed-Abstract-Conference.html">Paper</a>
              </td>
              </tr>
            </tbody>
        </table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tbody>
              <tr>
                  <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/papers/verl.png' width="80%">
                  </td>
              <td style="padding:10px;width:70%;vertical-align:middle">
                  <a href="https://proceedings.neurips.cc/paper_files/paper/2024/hash/935de67d1a033fd517cb49d192b5c008-Abstract-Datasets_and_Benchmarks_Track.html">
                  <papertitle>
                   V-PETL bench: A Unified Visual Parameter-efficient Transfer Learning Benchmark
                  </papertitle>
                  </a>
                  <br>
                Yi Xin, Siqi Luo, Xuyang Liu, Haodi Zhou, Xinyu Cheng, Christina E Lee, Junlong Du, Haozhe Wang, MingCai Chen, Ting Liu, Guimin Hu, <strong>Zhongwei Wan</strong>, Aoxue Li, Mingyang Yi, Xiaohong Liu
               <br>
                  <strong style="color:#1e6fff;">NeurIPS 2024</strong>
                  <br>
                   <a href="https://v-petl-bench.github.io/">Code</a>
                  /
                  <a href="https://proceedings.neurips.cc/paper_files/paper/2024/hash/935de67d1a033fd517cb49d192b5c008-Abstract-Datasets_and_Benchmarks_Track.html">Paper</a>
              </td>
              </tr>
            </tbody>
        </table>


        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tbody>
              <tr>
                  <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/papers/clinical bench.png' width="80%">
                  </td>
              <td style="padding:10px;width:70%;vertical-align:middle">
                  <a href="https://proceedings.neurips.cc/paper_files/paper/2024/hash/935de67d1a033fd517cb49d192b5c008-Abstract-Datasets_and_Benchmarks_Track.html">
                  <papertitle>
                   ClinicalBench: Can LLMs Beat Traditional ML Models in Clinical Prediction?
                  </papertitle>
                  </a>
                  <br>
                Canyu Chen, Jian Yu, Shan Chen, Che Liu, <strong>Zhongwei Wan</strong>, Danielle Bitterman, Fei Wang, Kai Shu
               <br>
                  <strong style="color:#1e6fff;">NeurIPS 2024</strong>@ GenAI4Health
                  <br>
                   <a href="https://clinicalbench.github.io/">Code</a>
                  /
                  <a href="https://proceedings.neurips.cc/paper_files/paper/2024/hash/935de67d1a033fd517cb49d192b5c008-Abstract-Datasets_and_Benchmarks_Track.html">Paper</a>
              </td>
              </tr>
            </tbody>
        </table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tbody>
              <tr>
                  <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/papers/efficient LLM.png' width="80%">
                  </td>
              <td style="padding:10px;width:70%;vertical-align:middle">
                  <a href="https://openreview.net/forum?id=bsCCJHbO8A">
                  <papertitle>
                   Efficient Large Language Models: A Survey
                  </papertitle>
                  </a>
                  <br>
                <strong>Zhongwei Wan</strong>, Xin Wang, Che Liu, Samiul Alam, Yu Zheng, Jiachen Liu, Zhongnan Qu, Shen Yan, Yi Zhu, Quanlu Zhang, Mosharaf Chowdhury, Mi Zhang
               <br>
                  <strong style="color:#1e6fff;">TMLR 2024</strong>
                  <br>
                   <a href="https://clinicalbench.github.io/">Code</a>
                  /
                  <a href="https://openreview.net/forum?id=bsCCJHbO8A">Paper</a>
              </td>
              </tr>
            </tbody>
        </table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tbody>
              <tr>
                  <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/papers/ICML24.png' width="80%">
                  </td>
              <td style="padding:10px;width:70%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2403.06659">
                  <papertitle>
                  Zero-Shot ECG Classification with Multimodal Learning and Test-time Clinical Knowledge Enhancement
                  </papertitle>
                  </a>
                  <br>
                Che Liu, <strong>Zhongwei Wan</strong>, Cheng Ouyang, Anand Shah, Wenjia Bai, Rossella Arcucci
               <br>
                  <strong style="color:#1e6fff;">ICML 2024</strong>
                  <br>
                   <a href="https://github.com/cheliu-computation/MERL-ICML2024">Code</a>
                  /
                  <a href="https://arxiv.org/abs/2403.06659">Paper</a>
              </td>
              </tr>
            </tbody>
        </table>

           <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tbody>
              <tr>
                  <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/papers/ai4sci.png' width="80%">
                  </td>
              <td style="padding:10px;width:70%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2406.03403">
                  <papertitle>
                  Structure-based Drug Design Benchmark: Do 3D Methods Really Dominate?
                  </papertitle>
                  </a>
                  <br>
                Kangyu Zheng, Yingzhou Lu, Zaixi Zhang, <strong>Zhongwei Wan</strong>, Yao Ma, Marinka Zitnik, Tianfan Fu
               <br>
                  <strong style="color:#1e6fff;">ICML 2024</strong>@AI4Science
                  <br>
                   <a href="https://github.com/zkysfls/2024-sbdd-benchmark">Code</a>
                  /
                  <a href="https://arxiv.org/abs/2406.03403">Paper</a>
              </td>
              </tr>
            </tbody>
        </table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tbody>
              <tr>
                  <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/papers/aiot.png' width="80%">
                  </td>
              <td style="padding:10px;width:70%;vertical-align:middle">
                  <a href="https://www.computer.org/csdl/magazine/ic/2024/05/10637270/1ZsL8VAqHTO">
                  <papertitle>
                  The Internet of Things in the Era of Generative AI: Vision and Challenges
                  </papertitle>
                  </a>
                  <br>
                Xin Wang, <strong>Zhongwei Wan</strong>, Arvin Hekmati, Mingyu Zong, Samiul Alam, Mi Zhang, Bhaskar Krishnamachari
               <br>
                  <strong style="color:#1e6fff;">IEEE Internet Computing Magazine 2024</strong> <strong><font color='#b70000'> (Best Paper AwardüèÜ)</strong></font>
                  <br>
                   <a href="https://github.com/AIoT-MLSys-Lab/AIoT-Survey">Code</a>
                  /
                  <a href="https://www.computer.org/csdl/magazine/ic/2024/05/10637270/1ZsL8VAqHTO">Paper</a>
              </td>
              </tr>
            </tbody>
        </table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tbody>
              <tr>
                  <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/papers/icassp.png' width="80%">
                  </td>
              <td style="padding:10px;width:70%;vertical-align:middle">
                  <a href="https://ieeexplore.ieee.org/abstract/document/10446742">
                  <papertitle>
                  ETP: Learning Transferable ECG Representations via ECG-Text Pre-training
                  </papertitle>
                  </a>
                  <br>
                Che Liu<strong>*</strong>, <strong>Zhongwei Wan*</strong>, Sibo Cheng, Mi Zhang, Rossella Arcucci
               <br>
                  <strong style="color:#1e6fff;">ICASSP 2024</strong> <strong>*Co-first Author</strong>
                  <br>
                   <a href="https://github.com/AIoT-MLSys-Lab/xxx">Code</a>
                  /
                  <a href="https://ieeexplore.ieee.org/abstract/document/10446742">Paper</a>
              </td>
              </tr>
            </tbody>
        </table>




         <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tbody>
              <tr>
                  <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/papers/medunic.png' width="80%">
                  </td>
              <td style="padding:10px;width:70%;vertical-align:middle">
                  <a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/af38fb8e90d586f209235c94119ba193-Abstract-Conference.html">
                  <papertitle>
                  Med-UniC: Unifying Cross-Lingual Medical Vision-Language Pre-Training by Diminishing Bias
                  </papertitle>
                  </a>
                  <br>
                <strong>Zhongwei Wan</strong>, Che Liu, Mi Zhang, Jie Fu, Benyou Wang, Sibo Cheng, Lei Ma, C√©sar Quilodr√°n-Casas, Rossella Arcucci
               <br>
                  <strong style="color:#1e6fff;">NeurIPS 2023</strong>
                  <br>
                   <a href="https://github.com/SUSTechBruce/Med-UniC">Code</a>
                  /
                  <a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/af38fb8e90d586f209235c94119ba193-Abstract-Conference.html">Paper</a>
              </td>
              </tr>
            </tbody>
        </table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tbody>
              <tr>
                  <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/papers/acm tois.png' width="80%">
                  </td>
              <td style="padding:10px;width:70%;vertical-align:middle">
                  <a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/af38fb8e90d586f209235c94119ba193-Abstract-Conference.html">
                  <papertitle>
                  Spatio-Temporal Contrastive Learning Enhanced GNNs for Session-based Recommendation
                  </papertitle>
                  </a>
                  <br>
                <strong>Zhongwei Wan</strong>, Xin Liu, Benyou Wang, Jiezhong Qiu, Boyu Li, Ting Guo, Guangyong Chen, Yang Wang
               <br>
                  <strong style="color:#1e6fff;">ACM TOIS 2023</strong>
                  <br>
                   <a href="https://github.com/SUSTechBruce/Med-UniC">Code</a>
                  /
                  <a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/af38fb8e90d586f209235c94119ba193-Abstract-Conference.html">Paper</a>
              </td>
              </tr>
            </tbody>
        </table>




        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tbody>
              <tr>
                  <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/papers/GMAP.png' width="80%">
                  </td>
              <td style="padding:10px;width:70%;vertical-align:middle">
                  <a href="https://aclanthology.org/2022.emnlp-main.441/">
                  <papertitle>
                  G-map: general memory-augmented pre-trained language model for domain tasks
                  </papertitle>
                  </a>
                  <br>
                <strong>Zhongwei Wan</strong>, Yichun Yin, Wei Zhang, Jiaxin Shi, Lifeng Shang, Guangyong Chen, Xin Jiang, Qun Liu
               <br>
                  <strong style="color:#1e6fff;">EMNLP 2022</strong>
                  <br>
                   <a href="https://github.com/SUSTechBruce/G-MAP">Code</a>
                  /
                  <a href="https://aclanthology.org/2022.emnlp-main.441/">Paper</a>
              </td>
              </tr>
            </tbody>
        </table>


        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tbody>
              <tr>
                  <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/papers/mathword.png' width="80%">
                  </td>
              <td style="padding:10px;width:70%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2210.15373">
                  <papertitle>
                 Self-consistent reasoning for solving math word problems
                  </papertitle>
                  </a>
                  <br>
                Jing Xiong, <strong>Zhongwei Wan</strong>, Xiping Hu, Min Yang, Chengming Li
               <br>
                  <em>Technical Report 2022<em>
                  <br>
                  <a href="https://arxiv.org/abs/2210.15373">Paper</a>
              </td>
              </tr>
            </tbody>
        </table>











      


        <!-- Teaching Experience -->
        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:16px;width:100%;vertical-align:middle">
              <h2>Teaching</h2>
              <ul>
                <li>
                    <strong>Teaching Assistant</strong> @ The Ohio State University.
                    <br>
                    Fall 2024: Introduction to Digital Logic (ECE 2060)
                </li>
                <li>
                    <strong>Teaching Assistant</strong> @ The Ohio State University.
                    <br>
                    Fall 2024: Systems II: Introduction to Operating Systems (CSE 2431)
                </li>
              </ul>
            </td>
            </tr>
        </tbody></table> -->

        <!-- Contact -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:16px;width:100%;vertical-align:middle">
              <h2>Contact</h2>
                <p>
                    <strong>Email:</strong> wan.512 [at] osu [dot] edu
            </td>
            </tr>
        </tbody></table>

          <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <video autoplay muted loop playsinline width="300px">
                    <source src="vids/main/adaptigraph.mp4" type="video/mp4">
                  </video>
                </td>
                <td style="padding:20px;width:70%;vertical-align:middle">
                  <a href="https://robopil.github.io/adaptigraph/">
                    <papertitle>
                      AdaptiGraph: Material-Adaptive Graph-Based Neural Dynamics for Robotic Manipulation
                    </papertitle>
                  </a>
                  <br>
                  <strong><a href="https://kywind.github.io/">Kaifeng Zhang</a>*</strong>,
                  <a href="https://scholar.google.com/citations?user=pFeywbYAAAAJ&hl=en">Baoyu Li</a>*, 
                  <a href="https://kkhauser.web.illinois.edu/">Kris Hauser</a>, 
                  <a href="https://yunzhuli.github.io/">Yunzhu Li</a>
                  <br>
                  Robotics: Science and Systems (RSS), 2024
                  <br>
                  ICRA RMDO Workshop, 2024 <strong><font color='#b70000'>(Best Abstract Award, Top 1)</strong></font>
                  <br>
                  <a href="https://robopil.github.io/adaptigraph/">website</a>
                  /
                  <a href="https://arxiv.org/abs/2407.07889">arXiv</a>
                  /
                  <a href="https://arxiv.org/pdf/2407.07889.pdf">pdf</a>
                  /
                  <a href="https://github.com/Boey-li/AdaptiGraph">code</a>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <img src='images/4drecons.jpg' width="300px">
                </td>
                <td style="padding:20px;width:70%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2406.10167">
                    <papertitle>
                      4DRecons: 4D Neural Implicit Deformable Objects Reconstruction from a single RGB-D Camera with Geometrical and Topological Regularizations
                    </papertitle>
                  </a>
                  <br>
                  <a href="https://xy-cong.github.io/">Xiaoyan Cong</a>,
                  <a href="https://yanghtr.github.io/">Haitao Yang</a>,
                  <a href="https://scholar.google.com/citations?user=ppaEV-8AAAAJ&hl=en">Liyan Chen</a>,
                  <strong><a href="https://kywind.github.io/">Kaifeng Zhang</a></strong>,
                  <a href="https://ericyi.github.io/">Li Yi</a>,
                  <a href="https://www.cs.utexas.edu/~bajaj/cvc/index.shtml">Chandrajit Bajaj</a>,
                  <a href="https://www.cs.utexas.edu/~huangqx/index.html">Qixing Huang</a>
                  <br>
                  Arxiv, 2024
                  <br>
                  <a href="https://arxiv.org/abs/2406.10167">arXiv</a>
                  /
                  <a href="https://arxiv.org/pdf/2406.10167.pdf">pdf</a>
                </td>
              </tr>
            </tbody>
          </table> -->

          
          <!-- Visit Statistic -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:16px;width:100%;vertical-align:middle;text-align:center;">
                  <h2>Visit Statistic</h2>
                  <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=080808&w=300&t=tt&d=tw2_506UT1zXRgjtkp68qPMgFK-ObI3n7spI-i0b1Rg&co=ffffff&ct=808080&cmo=3acc3a&cmn=ff5353'></script>
                </td>
              </tr>
            </tbody>
          </table>


        </td>
      </tr>
    </table>

    
    <button id="backToTop" class="back-to-top">
      <svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <polyline points="18 15 12 9 6 15"></polyline>
      </svg>
    </button>

    <script>
      const backToTopButton = document.getElementById('backToTop');
      window.addEventListener('scroll', () => {
          if (window.scrollY > 300) {
              backToTopButton.classList.add('show');
          } else {
              backToTopButton.classList.remove('show');
          }
      });

      backToTopButton.addEventListener('click', () => {
          window.scrollTo({
              top: 0,
              behavior: 'smooth'
          });
      });
    </script>

    <script src="./js/bootstrap.bundle.min.js"></script>
  </body>
</html>
